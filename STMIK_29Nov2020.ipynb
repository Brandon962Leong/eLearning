{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center> https://bit.ly/stmik-29nov2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img alt=\"\" src=\"images/Cover_stmiknm_29112020.png\"/></center> \n",
    "\n",
    "## <center><font color=\"blue\">(C) Taufik Sutanto - 2020</font></center>\n",
    "<center> <strong>tau-data Indonesia</strong> ~ https://tau-data.id</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <font color=\"blue\">Outline</font>: Workshop Machine Learning via Python\n",
    "\n",
    "* Pengenalan\n",
    "* Referensi\n",
    "* Antara Data Mining-Science & Machine Learning-AI\n",
    "* Data Mining dan Software Engineering dalam CRISP-DM\n",
    "* Studi Kasus Sentimen Analysis Bahasa Indonesia\n",
    " - Sumber Data & Hasil Sebelumnya\n",
    " - EDA: PreProcessing & Text Analytics\n",
    " - Representasi Dokumen\n",
    " - Pemodelan & Evaluasi\n",
    " - Optimal Parameter & Cross validasi\n",
    " - Pemilihan Model\n",
    " - Ensemble Model\n",
    "* Software Engineering in Data Science:\n",
    " - Konsep SE di Data Science/Big Data\n",
    " - Pengenalan Functional Programming (Map-Reduce)\n",
    "* Trend Riset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img alt=\"\" src=\"images/bio_TS.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# References:\n",
    "* Menzies, T., Williams, L., & Zimmermann, T. (2016). Perspectives on data science for software engineering. Morgan Kaufmann.\n",
    "* Cruz, L. P. (2017). When Data Science Becomes Software Engineering. In KEOD (pp. 226-232).\n",
    "* Ramachandran, M. (Ed.). (2009). Handbook of Research on Software Engineering and Productivity Technologies: Implications of Globalization: Implications of Globalization. IGI Global.\n",
    "* https://www.kdnuggets.com/2020/06/software-engineering-fundamentals-data-scientists.html\n",
    "* http://hiphoff.com/software-engineering-for-data-scientists/\n",
    "* https://medium.com/swlh/software-engineering-tips-and-best-practices-for-data-science-5d85dbcf87fd\n",
    "* https://github.com/A2Amir/Software-Engineering-Practices-in-Data-Science.\n",
    "* https://meganbloemsma.com/2020/06/24/the-future-of-data-science-is-software-engineering/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img alt=\"\" src=\"images/DS-SE.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Penyamaan Persepsi Berbagai Cabang Ilmu Terkait Data\n",
    "\n",
    "* **Statistika**: 1 hipotesis, 1 dataset  \n",
    " - bisa terdiri dari beberapa model, hipotesis diketahui di awal\n",
    "* **Data Mining**: N hipotesis, 1-N dataset, Fokus ke Insight/Informasi (bukan akurasi seperti ML).\n",
    " - Normalnya beberapa model, hipotesis tidak ditentukan diawal, Stat & ML digunakan sebagai \"tools\" \n",
    "* **AI**: Sebuah sistem ML (superset ML)\n",
    " - Fokus ke akurasi & Automatisasi\n",
    "* **Machine Learning**: Subset dari AI, biasanya hanya 1 task\n",
    " - Fokus ke akurasi & Automatisasi \n",
    "* **Data Science**: Statistik pada data terstruktur & Tidak terstruktur, Seperti Data Mining fokus ke Insight/Informasi.\n",
    " - Model/Algoritma lebih dari sekedar Tools, biasanya perlu modifikasi, sehingga perlu skill pemrograman.\n",
    "* **Big Data**: Syarat utama Data & komputasi harus terdisribusi, tidak ada ukuran minimal data.\n",
    " - Biasanya pada data tidak terstruktur dan-atau streaming data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Embedded Analytics: Software Engineering ke Data Science\n",
    "\n",
    "<img alt=\"\" src=\"images/sdlc-crispdm.png\"/>\n",
    "\n",
    "* Ada banyak metodologi SD (e.g. Kanban, Scrum, Agile, waterfall, etc), tapi dasarnya ada 5 (SDLC di Gambar)\n",
    "* https://tdan.com/making-crisp-dm-work-for-embedded-analytics/25884\n",
    "* Cruz, L. P. (2017). When Data Science Becomes Software Engineering. In KEOD (pp. 226-232)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Studi Kasus Sentimen Analysis Bahasa Indonesia menggunakan Python\n",
    "\n",
    "### Fokus dari Studi Kasus ini adalah <font color=\"blue\">\"Computational Thinking\"</font> Machine Learning untuk riset. Bukan penggunaan Python atau model di ML."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Mengapa Python?\n",
    "\n",
    "* Free - Open Source\n",
    "* Growing in needs & popularity + Community support\n",
    "* Portable - Multi Platform\n",
    "* “Fast” https://tau-data.id/python-showdown/\n",
    "* Interpreter, JIT, & Compiler sekaligus: https://tau-data.id/cython-semudah-python/\n",
    "* Rich libraries/Modules (termasuk Matematika, Statistika, Data Science/Machine Learning, Big Data, etc)\n",
    "* Easier to learn (learning curve not steep)\n",
    "* Procedural, OOP, & Functional Programming Paradigm \n",
    "\n",
    "<img alt=\"\" src=\"images/1_Python_VS_TheRest.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Google Colab\n",
    "\n",
    "* Cocok untuk komputer yang memiliki spesifikasi yang relatif minim. \n",
    "* Free with GPU support (penting saat nanti belajar Data Science/Machine Learning, terutama Deep Learning)\n",
    "* Google Colab dapat digunakan untuk memudahkan instalasi dan menyiapkan environment dalam menjalankan scripts Python untuk berbagai kegiatan seperti workshop, presentasi, kuliah, dsb:\n",
    "* **Warning**: Jupyter Notebook tidak disarankan untuk Software Engineering\n",
    "* https://colab.research.google.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Sentimen Analysis Bahasa Indonesia: Sumber Data & Hasil Sebelumnya\n",
    "\n",
    "* Sumber Data: https://www.researchgate.net/publication/338409000_Dataset_Indonesia_untuk_Analisis_Sentimen\n",
    "* Dataset di GitHub: https://github.com/ridife/dataset-idsa\n",
    "* Hasil Sebelumnya:\n",
    "\n",
    "<img alt=\"\" src=\"images/sentimenAnalisis_hasil_sebelumnya.png\" />\n",
    "\n",
    "## Mengambil Data Media Sosial Sendiri? https://tau-data.id/sma-01/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Loading Modules & Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Jalankan Cell ini \"HANYA\" jika anda menggunakan Google Colab\n",
    "# Jika di jalankan di komputer local, silahkan lihat NLPTM-02 untuk instalasinya.\n",
    "import warnings; warnings.simplefilter('ignore')\n",
    "import nltk\n",
    "\n",
    "!wget https://raw.githubusercontent.com/taudata-indonesia/eLearning/master/taudataNlpTm.py\n",
    "!mkdir data\n",
    "!wget -P data/ https://raw.githubusercontent.com/taudata-indonesia/eLearning/master/data/slang.txt\n",
    "!wget -P data/ https://raw.githubusercontent.com/taudata-indonesia/eLearning/master/data/stopwords_id.txt\n",
    "!wget -P data/ https://raw.githubusercontent.com/taudata-indonesia/eLearning/master/data/stopwords_en.txt\n",
    "!wget -P data/ https://raw.githubusercontent.com/taudata-indonesia/eLearning/master/data/kata_dasar.txt\n",
    "!wget -P data/ https://raw.githubusercontent.com/taudata-indonesia/eLearning/master/data/wn-ind-def.tab\n",
    "!wget -P data/ https://raw.githubusercontent.com/taudata-indonesia/eLearning/master/data/wn-msa-all.tab\n",
    "!wget -P data/ https://raw.githubusercontent.com/taudata-indonesia/eLearning/master/data/ind_SA.csv\n",
    "!wget -P data/ https://raw.githubusercontent.com/taudata-indonesia/eLearning/master/data/all_indo_man_tag_corpus_model.crf.tagger\n",
    "\n",
    "!pip install spacy python-crfsuite unidecode textblob sastrawi\n",
    "!python -m spacy download en\n",
    "!python -m spacy download xx\n",
    "!python -m spacy download en_core_web_sm\n",
    "\n",
    "nltk.download('popular')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentimen</th>\n",
       "      <th>Tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>lagu bosan apa yang aku save ni huhuhuhuhuhuhu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1</td>\n",
       "      <td>kita lanjutkan saja diam ini hingga kau dan ak...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>doa rezeki tak putus inna haa zaa larizquna ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>makasih loh ntar kita bagi hasil aku 99 9 sisa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1</td>\n",
       "      <td>aku tak faham betul jenis orang malaysia yang ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentimen                                              Tweet\n",
       "0        -1  lagu bosan apa yang aku save ni huhuhuhuhuhuhu...\n",
       "1        -1  kita lanjutkan saja diam ini hingga kau dan ak...\n",
       "2         1  doa rezeki tak putus inna haa zaa larizquna ma...\n",
       "3         1  makasih loh ntar kita bagi hasil aku 99 9 sisa...\n",
       "4        -1  aku tak faham betul jenis orang malaysia yang ..."
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk, warnings; warnings.simplefilter('ignore')\n",
    "import pandas as pd, taudataNlpTm as tau\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load DataFile CSV\n",
    "dataSA = pd.read_csv('data/ind_SA.csv') # run locally\n",
    "dataSA.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentimen</th>\n",
       "      <th>Tweet</th>\n",
       "      <th>tweet_cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>lagu bosan apa yang aku save ni huhuhuhuhuhuhu...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1</td>\n",
       "      <td>kita lanjutkan saja diam ini hingga kau dan ak...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>doa rezeki tak putus inna haa zaa larizquna ma...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>makasih loh ntar kita bagi hasil aku 99 9 sisa...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1</td>\n",
       "      <td>aku tak faham betul jenis orang malaysia yang ...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentimen                                              Tweet tweet_cleaned\n",
       "0        -1  lagu bosan apa yang aku save ni huhuhuhuhuhuhu...              \n",
       "1        -1  kita lanjutkan saja diam ini hingga kau dan ak...              \n",
       "2         1  doa rezeki tak putus inna haa zaa larizquna ma...              \n",
       "3         1  makasih loh ntar kita bagi hasil aku 99 9 sisa...              \n",
       "4        -1  aku tak faham betul jenis orang malaysia yang ...              "
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataSA['tweet_cleaned'] = ''\n",
    "dataSA.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# PreProcessing\n",
    "\n",
    "### Detail lebih lanjut di https://tau-data.id/nlptm-01/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10806it [00:10, 1062.16it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentimen</th>\n",
       "      <th>Tweet</th>\n",
       "      <th>tweet_cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>lagu bosan apa yang aku save ni huhuhuhuhuhuhu...</td>\n",
       "      <td>lagu bosan apa yang aku save ni huhuhuhuhuhuhu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1</td>\n",
       "      <td>kita lanjutkan saja diam ini hingga kau dan ak...</td>\n",
       "      <td>kita lanjutkan saja diam ini hingga kau dan ak...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>doa rezeki tak putus inna haa zaa larizquna ma...</td>\n",
       "      <td>doa rezeki tak putus inna haa zaa larizquna ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>makasih loh ntar kita bagi hasil aku 99 9 sisa...</td>\n",
       "      <td>makasih loh ntar kita bagi hasil aku 99 sisany...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1</td>\n",
       "      <td>aku tak faham betul jenis orang malaysia yang ...</td>\n",
       "      <td>aku tak faham betul jenis orang malaysia yang ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentimen                                              Tweet  \\\n",
       "0        -1  lagu bosan apa yang aku save ni huhuhuhuhuhuhu...   \n",
       "1        -1  kita lanjutkan saja diam ini hingga kau dan ak...   \n",
       "2         1  doa rezeki tak putus inna haa zaa larizquna ma...   \n",
       "3         1  makasih loh ntar kita bagi hasil aku 99 9 sisa...   \n",
       "4        -1  aku tak faham betul jenis orang malaysia yang ...   \n",
       "\n",
       "                                       tweet_cleaned  \n",
       "0  lagu bosan apa yang aku save ni huhuhuhuhuhuhu...  \n",
       "1  kita lanjutkan saja diam ini hingga kau dan ak...  \n",
       "2  doa rezeki tak putus inna haa zaa larizquna ma...  \n",
       "3  makasih loh ntar kita bagi hasil aku 99 sisany...  \n",
       "4  aku tak faham betul jenis orang malaysia yang ...  "
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopId, lemmaId = tau.LoadStopWords(lang='id') \n",
    "for i, d in tqdm(dataSA.iterrows()):\n",
    "    doc = tau.cleanText(d.Tweet, lemma=lemmaId, stops = None, symbols_remove = True, min_charLen = 2, fixTag= True)\n",
    "    dataSA.at[i, \"tweet_cleaned\"] = doc\n",
    "    \n",
    "dataSA.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>Text Analytics</h2>\n",
    "\n",
    "<ul>\n",
    "\t<li>Tidak seperti data terstruktur, data tidak terstruktur seperti teks termasuk salah satu data yang cukup sulit untuk divisualisasikan.<br />\n",
    "\t<img alt=\"\" src=\"images/11_charts.jpg\" style=\"height:150px; width:276px\" /></li>\n",
    "\t<li>Namun terdapat Tools seperti Voyant yang dapat membantu dalam visualisasi sekaligus analisis.<br />\n",
    "\t<img alt=\"\" src=\"images/11_voyant.png\" style=\"height:118px; width:426px\" /></li>\n",
    "</ul>\n",
    "\n",
    "### Voyant dapat digunakan dalam 2 cara:\n",
    "\n",
    "* Online: https://voyant-tools.org/\n",
    "* Offline di komputer kita [Java Based]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Done'"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pertama-tama kita perlu menyimpan tweet yang telah dilakukan preprocessing diatas sebagai file teks biasa\n",
    "# Menggunakan cara yang dibahas di hari pertama\n",
    "file = 'data/dataTweet_SA.txt'\n",
    "with open(file, 'w') as f:\n",
    "    for i, d in dataSA.iterrows():\n",
    "        f.write(d.tweet_cleaned+'\\n')\n",
    "'Done'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Catatan Text Analytics\n",
    "\n",
    "* Dalam Kasus nyata TA dilakukan per kategori (i.e. Pos, Neg, Net)\n",
    "* TA di seluruh data dapat menjadi gambaran seberapa bersih data.\n",
    "* Insight sederhana dapat dihasilkan dari SA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# \"Induktif Bias\" di Machine Learning\n",
    "\n",
    "* Seluruh Model di Machine Learning di Optimasi berdasarkan suatu fungsi tujuan (objective function). Biasanya adalah fungsi error atau Loss Function.\n",
    "* Tujuan Machine Learning adalah memperoleh performa model ketika di deploy, bukan pada data kita saat ini.\n",
    "\n",
    "<img alt=\"\" src=\"images/inductive_biases_.png\" />\n",
    "<img alt=\"\" src=\"images/low-high-bias-variance.png\" />\n",
    "\n",
    "## Mengingatkan kembali di <font color=\"blue\">Data Mining interpretasi lebih penting dari akurasi</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Solusi? Pisah Data menjadi 2 (Train-Test) atau jika datanya cukup besar menjadi 3 (Train-Test-Validation)\n",
    "* Hati-hati dalam memisahkan data, selalu gunakan Stratified Sampling. Apa maksudnya & mengapa?\n",
    "\n",
    "<img alt=\"\" src=\"images/train-test-data.png\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8644,) (2162,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "seed = 99 # Biasakan menggunakan ini\n",
    "testSize = 0.2\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(dataSA[\"tweet_cleaned\"], dataSA[\"sentimen\"], \n",
    "                                                    test_size=testSize, random_state = seed)\n",
    "print(x_train.shape, x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Representasi Dokumen Vector Space Model (VSM): tf-idf\n",
    "\n",
    "<img alt=\"\" src=\"images/toydata_vsm.png\" />\n",
    "\n",
    "* Menurut http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\n",
    "* default formula tf-idf yang digunakan sk-learn adalah:\n",
    "* $tfidf = tf * log(\\frac{N}{df+1})$ ==> Smooth IDF\n",
    "* namun kita merubahnya menjadi:\n",
    "* $tfidf = tf * log(\\frac{N}{df})$ ==> Non Smooth IDF\n",
    "* $tfidf = tf * log(\\frac{N}{df+1})$ ==> linear_tf, Smooth IDF\n",
    "* $tfidf = (1+log(tf)) * log(\\frac{N}{df})$ ==> sublinear_tf, Non Smooth IDF\n",
    "* NGram, frase kata, dan Tokenisasi : https://tau-data.id/nlptm-02/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8644, 20681) (2162, 20681)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vsm = TfidfVectorizer(lowercase=True, smooth_idf= True, sublinear_tf=True, \n",
    "                                   ngram_range=(1, 2), max_df=0.90, min_df=2)\n",
    "\n",
    "x_train = vsm.fit_transform(x_train) # \"Fit_Transform\"\n",
    "x_test = vsm.transform(x_test) # Perhatikan disini hanya \"Transform\"\n",
    "\n",
    "print(x_train.shape, x_test.shape) # Jumlah kolom Sama ==> ini penting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Document Classification ~ Sentiment Analysis\n",
    "\n",
    "## https://tau-data.id/slcm-01/\n",
    "## https://tau-data.id/slcm-02/\n",
    "## https://tau-data.id/slcm-03/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3>k-Nearest Neighbour</h3>\n",
    "<ul>\n",
    "\t<li>Classifier yang paling sederhana, namun dapat juga digunakan untuk regresi (dan bahkan clustering).</li>\n",
    "\t<li>Sering disebut sebagai <u><strong>Instance based Learner</strong></u></li>\n",
    "    <li>Tidak memiliki \"persamaan\", pendekatannya lebih ke algoritmik berdasarkan konsep jarak/similarity</li>\n",
    "    <li>Mirip konsep DBSCAN</li>\n",
    "</ul>\n",
    "<img alt=\"\" src=\"images/6_kNN.JPG\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<dl>\n",
    "\t<dt><strong>Pros:</strong></dt>\n",
    "\t<dd>\n",
    "\t<ul>\n",
    "\t\t<li>Relatif cepat (efisien) untuk data yang tidak terlalu besar</li>\n",
    "\t\t<li>Sederhana, mudah untuk diimplementasikan</li>\n",
    "\t\t<li>Mudah untuk di modifikasi: Berbagai macam formula jarak/similaritas</li>\n",
    "\t\t<li>Menangani data Multiclass dengan mudah</li>\n",
    "\t\t<li>Akurasi cukup baik jika data representatif</li>\n",
    "\t</ul>\n",
    "\t</dd>\n",
    "\t<dt><strong>Cons:</strong></dt>\n",
    "\t<dd>\n",
    "\t<ul>\n",
    "\t\t<li>Menemukan&nbsp;nearest neighbours tidak efisien untuk data besar</li>\n",
    "\t\t<li>Storage of data</li>\n",
    "\t\t<li>Meyakinkan rumus jarak yang tepat</li>\n",
    "\t</ul>\n",
    "\t</dd>\n",
    "</dl>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Akurasi =  0.4592969472710453\n",
      "[[111 360  90]\n",
      " [143 767 193]\n",
      " [ 66 317 115]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.35      0.20      0.25       561\n",
      "           0       0.53      0.70      0.60      1103\n",
      "           1       0.29      0.23      0.26       498\n",
      "\n",
      "    accuracy                           0.46      2162\n",
      "   macro avg       0.39      0.37      0.37      2162\n",
      "weighted avg       0.43      0.46      0.43      2162\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import neighbors\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "\n",
    "model = neighbors.KNeighborsClassifier()\n",
    "kNN = model.fit(x_train, y_train)\n",
    "y_kNN = kNN.predict(x_test)\n",
    "\n",
    "print('Akurasi = ', accuracy_score(y_test, y_kNN))\n",
    "print(confusion_matrix(y_test, y_kNN))\n",
    "print(classification_report(y_test, y_kNN))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Support Vector Machine: Soft Margin\n",
    "\n",
    "<img alt=\"\" src=\"images/6_SVM.jpg\" style=\"height: 262px ; width: 232px\" />\n",
    "<img alt=\"\" src=\"images/svm_opt.png\" style=\"width: 300px; height: 106px;\" />\n",
    "* Apakah efek outlier masih sama pada pemodelan ini? Kaitannya dengan nilai C?\n",
    "* C >>> ==> toleransi terhadap outlier <<<< dan sebaliknya"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<p><b>Pros</b></p>\n",
    "\n",
    "<ul>\n",
    "\t<li>Akurasinya Baik</li>\n",
    "\t<li>Bekerja dengan baik untuk sampel data yang relatif kecil</li>\n",
    "\t<li>Hanya bergantung pada SV ==&gt; meningkatkan efisiensi</li>\n",
    "\t<li>Convex ==&gt; Minimum Global ==&gt; Pasti Konvergen</li>\n",
    "</ul>\n",
    "\n",
    "<p><b>Cons</b></p>\n",
    "\n",
    "<ul>\n",
    "\t<li>Tidak efisien untuk data yang besar</li>\n",
    "\t<li>Akurasi terkadang rendah untuk multiklasifikasi (sulit mendapatkan hubungan antar kategori di modelnya)</li>\n",
    "\t<li>Tidak robust terhadap noise</li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Akurasi =  0.5952821461609621\n",
      "[[174 365  22]\n",
      " [ 77 973  53]\n",
      " [ 53 305 140]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.57      0.31      0.40       561\n",
      "           0       0.59      0.88      0.71      1103\n",
      "           1       0.65      0.28      0.39       498\n",
      "\n",
      "    accuracy                           0.60      2162\n",
      "   macro avg       0.61      0.49      0.50      2162\n",
      "weighted avg       0.60      0.60      0.56      2162\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "dSVM = svm.SVC()\n",
    "dSVM.fit(x_train, y_train)\n",
    "y_SVM = dSVM.predict(x_test)\n",
    "print('Akurasi = ', accuracy_score(y_test, y_SVM))\n",
    "print(confusion_matrix(y_test, y_SVM))\n",
    "print(classification_report(y_test, y_SVM))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Jaringan Syaraf Tiruan - Artificial neural network (ANN)\n",
    "\n",
    "<img alt=\"\" src=\"images/Multiclass_ANN.png\" style=\"width: 600px; height: 468px;\" />\n",
    "<p>Neural Network - Empirical Analysis Parameter di ANN</p>\n",
    "<strong><a href=\"https://goo.gl/3rcnc9\" target=\"_blank\">https://goo.gl/3rcnc9</a></strong>\n",
    "\n",
    "<p>Mengapa dengan fungsi linear bisa membentuk &quot;boundary&quot; yang melengkung (kurva)?</p>\n",
    "<strong><a href=\"http://s.id/j6i\" target=\"_blank\">http://s.id/j6i</a></strong>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Neural Network VS Deep Learning\n",
    "<img alt=\"\" src=\"images/5_DeepLearning.png\" style=\"width: 600px; height: 676px;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<p><img alt=\"\" src=\"images/6_NN_when_to_use.JPG\" style=\"height:400px; width:499px\" /></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Akurasi =  0.5679925994449584\n",
      "[[260 200 101]\n",
      " [199 718 186]\n",
      " [ 84 164 250]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.48      0.46      0.47       561\n",
      "           0       0.66      0.65      0.66      1103\n",
      "           1       0.47      0.50      0.48       498\n",
      "\n",
      "    accuracy                           0.57      2162\n",
      "   macro avg       0.54      0.54      0.54      2162\n",
      "weighted avg       0.57      0.57      0.57      2162\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "NN = MLPClassifier()\n",
    "NN.fit(x_train, y_train)\n",
    "y_NN = NN.predict(x_test)\n",
    "\n",
    "print('Akurasi = ', accuracy_score(y_test, y_NN))\n",
    "print(confusion_matrix(y_test, y_NN))\n",
    "print(classification_report(y_test, y_NN))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Optimasi Parameter\n",
    "\n",
    "* Preprocessing di ML di optimalkan bergantung model.\n",
    "* Parameter tiap model di ML berbeda-beda dan nilai optimalnya berbeda pada setiap kasus.\n",
    "\n",
    "<img alt=\"\" src=\"images/rand_grid_search.png\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score, RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.pipeline import make_pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 162 candidates, totalling 810 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   16.3s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:   46.1s\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done 784 tasks      | elapsed:  2.8min\n",
      "[Parallel(n_jobs=-1)]: Done 810 out of 810 | elapsed:  2.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4846140457982563\n",
      "{'kneighborsclassifier__n_neighbors': 10, 'kneighborsclassifier__weights': 'distance', 'tfidfvectorizer__max_df': 0.5, 'tfidfvectorizer__min_df': 15, 'tfidfvectorizer__ngram_range': (1, 3), 'tfidfvectorizer__smooth_idf': True, 'tfidfvectorizer__sublinear_tf': True}\n"
     ]
    }
   ],
   "source": [
    "# Optimal parameter k-NN dengan GRIDSEARCH\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(dataSA[\"tweet_cleaned\"], dataSA[\"sentimen\"], \n",
    "                                                    test_size=testSize, random_state = seed)\n",
    "# Perhatikan kita pakai data awal : Text karena kita akan optimalkan preprocessing juga\n",
    "\n",
    "kCV = 5\n",
    "metric = 'accuracy'\n",
    "params = {}\n",
    "params['tfidfvectorizer__min_df'] = [5, 10, 15]\n",
    "params['tfidfvectorizer__max_df'] = [0.5, 0.75, 0.95]\n",
    "params['tfidfvectorizer__smooth_idf'] = [True] # [True, False]\n",
    "params['tfidfvectorizer__sublinear_tf'] = [True] # [True, False]\n",
    "params['tfidfvectorizer__ngram_range'] = [(1, 1), (1, 2), (1,3)]\n",
    "params['kneighborsclassifier__n_neighbors'] = [3, 5, 10]\n",
    "params['kneighborsclassifier__weights'] = ('distance', 'uniform')\n",
    "\n",
    "pipe = make_pipeline(TfidfVectorizer(), neighbors.KNeighborsClassifier())\n",
    "gridCV = GridSearchCV(pipe, params, cv=kCV, scoring=metric, verbose=1, n_jobs=-1) # , pre_dispatch='2*n_jobs', pre_dispatch min 2* n_jobs\n",
    "gridCV.fit(x_train, y_train)\n",
    "print(gridCV.best_score_)\n",
    "print(gridCV.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['memory', 'steps', 'svc', 'svc__C', 'svc__break_ties', 'svc__cache_size', 'svc__class_weight', 'svc__coef0', 'svc__decision_function_shape', 'svc__degree', 'svc__gamma', 'svc__kernel', 'svc__max_iter', 'svc__probability', 'svc__random_state', 'svc__shrinking', 'svc__tol', 'svc__verbose', 'tfidfvectorizer', 'tfidfvectorizer__analyzer', 'tfidfvectorizer__binary', 'tfidfvectorizer__decode_error', 'tfidfvectorizer__dtype', 'tfidfvectorizer__encoding', 'tfidfvectorizer__input', 'tfidfvectorizer__lowercase', 'tfidfvectorizer__max_df', 'tfidfvectorizer__max_features', 'tfidfvectorizer__min_df', 'tfidfvectorizer__ngram_range', 'tfidfvectorizer__norm', 'tfidfvectorizer__preprocessor', 'tfidfvectorizer__smooth_idf', 'tfidfvectorizer__stop_words', 'tfidfvectorizer__strip_accents', 'tfidfvectorizer__sublinear_tf', 'tfidfvectorizer__token_pattern', 'tfidfvectorizer__tokenizer', 'tfidfvectorizer__use_idf', 'tfidfvectorizer__vocabulary', 'verbose']\n"
     ]
    }
   ],
   "source": [
    "# Optimal parameter SVM dengan RandomSEARCH\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html\n",
    "\n",
    "pipeSVM = make_pipeline(TfidfVectorizer(), svm.SVC())\n",
    "print(sorted(pipeSVM.get_params().keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 150 out of 150 | elapsed: 11.7min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5967140741811794\n",
      "{'tfidfvectorizer__sublinear_tf': True, 'tfidfvectorizer__smooth_idf': True, 'tfidfvectorizer__ngram_range': (1, 3), 'tfidfvectorizer__min_df': 5, 'tfidfvectorizer__max_df': 0.75, 'svc__kernel': 'rbf', 'svc__gamma': 1.0, 'svc__decision_function_shape': 'ovr', 'svc__C': 10}\n"
     ]
    }
   ],
   "source": [
    "# Optimal parameter SVM dengan RandomizedSearch\n",
    "\n",
    "paramsSVM = {}\n",
    "paramsSVM['tfidfvectorizer__min_df'] = [5, 10, 30]\n",
    "paramsSVM['tfidfvectorizer__max_df'] = [0.5, 0.75, 0.95]\n",
    "paramsSVM['tfidfvectorizer__smooth_idf'] = [True] # [True, False]\n",
    "paramsSVM['tfidfvectorizer__sublinear_tf'] = [True] # [True, False]\n",
    "paramsSVM['tfidfvectorizer__ngram_range'] = [(1, 1), (1, 2), (1,3)]\n",
    "paramsSVM['svc__C'] = [0.1, 10, 100] #sp.stats.uniform(scale=1)\n",
    "paramsSVM['svc__gamma'] = [1.0, 0.1, 0.001]\n",
    "paramsSVM['svc__kernel'] = ['rbf', 'poly', 'sigmoid', 'linear']\n",
    "paramsSVM['svc__decision_function_shape'] = ['ovo', 'ovr']\n",
    "\n",
    "randsvmCV = RandomizedSearchCV(pipeSVM, paramsSVM, cv=kCV, scoring=metric, verbose=1, n_iter=30, random_state=seed, n_jobs=-1) # , pre_dispatch='2*n_jobs' pre_dispatch min 2* n_jobs\n",
    "randsvmCV.fit(x_train, y_train)\n",
    "print(randsvmCV.best_score_)\n",
    "print(randsvmCV.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['memory', 'mlpclassifier', 'mlpclassifier__activation', 'mlpclassifier__alpha', 'mlpclassifier__batch_size', 'mlpclassifier__beta_1', 'mlpclassifier__beta_2', 'mlpclassifier__early_stopping', 'mlpclassifier__epsilon', 'mlpclassifier__hidden_layer_sizes', 'mlpclassifier__learning_rate', 'mlpclassifier__learning_rate_init', 'mlpclassifier__max_fun', 'mlpclassifier__max_iter', 'mlpclassifier__momentum', 'mlpclassifier__n_iter_no_change', 'mlpclassifier__nesterovs_momentum', 'mlpclassifier__power_t', 'mlpclassifier__random_state', 'mlpclassifier__shuffle', 'mlpclassifier__solver', 'mlpclassifier__tol', 'mlpclassifier__validation_fraction', 'mlpclassifier__verbose', 'mlpclassifier__warm_start', 'steps', 'tfidfvectorizer', 'tfidfvectorizer__analyzer', 'tfidfvectorizer__binary', 'tfidfvectorizer__decode_error', 'tfidfvectorizer__dtype', 'tfidfvectorizer__encoding', 'tfidfvectorizer__input', 'tfidfvectorizer__lowercase', 'tfidfvectorizer__max_df', 'tfidfvectorizer__max_features', 'tfidfvectorizer__min_df', 'tfidfvectorizer__ngram_range', 'tfidfvectorizer__norm', 'tfidfvectorizer__preprocessor', 'tfidfvectorizer__smooth_idf', 'tfidfvectorizer__stop_words', 'tfidfvectorizer__strip_accents', 'tfidfvectorizer__sublinear_tf', 'tfidfvectorizer__token_pattern', 'tfidfvectorizer__tokenizer', 'tfidfvectorizer__use_idf', 'tfidfvectorizer__vocabulary', 'verbose']\n"
     ]
    }
   ],
   "source": [
    "# Optimal parameter ANN dengan RandomSEARCH\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html\n",
    "\n",
    "pipeNN = make_pipeline(TfidfVectorizer(), MLPClassifier())\n",
    "print(sorted(pipeNN.get_params().keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:  4.4min\n",
      "[Parallel(n_jobs=-1)]: Done 150 out of 150 | elapsed: 12.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5681411059700534\n",
      "{'tfidfvectorizer__sublinear_tf': True, 'tfidfvectorizer__smooth_idf': True, 'tfidfvectorizer__ngram_range': (1, 1), 'tfidfvectorizer__min_df': 10, 'tfidfvectorizer__max_df': 0.95, 'mlpclassifier__learning_rate': 'invscaling', 'mlpclassifier__hidden_layer_sizes': (5, 10), 'mlpclassifier__activation': 'logistic'}\n"
     ]
    }
   ],
   "source": [
    "paramsNN = {}\n",
    "paramsNN['tfidfvectorizer__min_df'] = [5, 10, 30]\n",
    "paramsNN['tfidfvectorizer__max_df'] = [0.5, 0.75, 0.95]\n",
    "paramsNN['tfidfvectorizer__smooth_idf'] = [True] # [True, False]\n",
    "paramsNN['tfidfvectorizer__sublinear_tf'] = [True] # [True, False]\n",
    "paramsNN['tfidfvectorizer__ngram_range'] = [(1, 1), (1, 2), (1,3)]\n",
    "paramsNN['mlpclassifier__hidden_layer_sizes'] = [(5,10), (20,30), (30,50)] \n",
    "paramsNN['mlpclassifier__learning_rate'] = ['constant', 'invscaling', 'adaptive']\n",
    "paramsNN['mlpclassifier__activation'] = ['logistic', 'tanh', 'relu' ]\n",
    "\n",
    "randNnCV = RandomizedSearchCV(pipeNN, paramsNN, cv=kCV, scoring=metric, verbose=1, n_iter=30, random_state=seed, n_jobs=-1) # , pre_dispatch='2*n_jobs' pre_dispatch min 2* n_jobs\n",
    "randNnCV.fit(x_train, y_train)\n",
    "print(randNnCV.best_score_)\n",
    "print(randNnCV.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Model Selection\n",
    "\n",
    "<img alt=\"\" src=\"images/grid_search_workflow.png\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.54338263, 0.52556665, 0.50694009, 0.54291953, 0.51376619,\n",
       "       0.52591394, 0.54812639, 0.52660879, 0.52973145, 0.51145338,\n",
       "       0.56062064, 0.56131495, 0.53192885, 0.56027254, 0.55865304,\n",
       "       0.55946443, 0.56038929, 0.54708466, 0.53655814, 0.51920513,\n",
       "       0.51550129, 0.5328537 , 0.52637664, 0.55923235, 0.5107594 ,\n",
       "       0.56814111, 0.53343468, 0.56258776, 0.56154569, 0.55252247])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn_score = gridCV.cv_results_['mean_test_score'][:30]\n",
    "svm_score = randsvmCV.cv_results_['mean_test_score'][:30]\n",
    "ann_score = randNnCV.cv_results_['mean_test_score'][:30]\n",
    "\n",
    "ann_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy kNN: 0.40 (+/- 0.07)\n",
      "Accuracy SVM: 0.52 (+/- 0.06)\n",
      "Accuracy NN: 0.54 (+/- 0.04)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns, matplotlib.pyplot as plt\n",
    "\n",
    "models = ['kNN', 'SVM', 'NN']\n",
    "scores = [knn_score, svm_score, ann_score]\n",
    "\n",
    "data = {m:s for m,s in zip(models, scores)}\n",
    "for name in data.keys():\n",
    "    print(\"Accuracy %s: %0.2f (+/- %0.2f)\" % (name, data[name].mean(), data[name].std() * 2))\n",
    "\n",
    "sns.boxplot(data=pd.DataFrame(data), orient='h')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2 id=\"Ensemble-Model\">Ensemble Model</h2>\n",
    "\n",
    "<ul>\n",
    "\t<li>What? a learning algorithms that construct a set of classifiers and then classify new data points by taking a (weighted) vote of their predictions.</li>\n",
    "\t<li>Why? Better prediction, More stable model</li>\n",
    "\t<li>How? Bagging &amp; Boosting</li>\n",
    "</ul>\n",
    "<img alt=\"\" src=\"images/Ensemble.png\" style=\"width: 500px; height: 213px;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## “meta-algorithms” : Bagging & Boosting\n",
    "* Ensemble https://www.youtube.com/watch?v=Un9zObFjBH0 \n",
    "* Bagging https://www.youtube.com/watch?v=2Mg8QD0F1dQ \n",
    "* Boosting https://www.youtube.com/watch?v=GM3CDQfQ4sw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<p><img alt=\"\" src=\"images/Bagging_VS_Boosting.png\" style=\"width: 500px; height: 185px;\" /></p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Akurasi k-NN =  0.4846140457982563\n",
      "Akurasi SVM =  0.5967140741811794\n",
      "Akurasi ANN =  0.5681411059700534\n",
      "Akurasi Ensemble =  0.5619796484736356\n"
     ]
    }
   ],
   "source": [
    "# Contoh Voting (Bagging) di Python\n",
    "# Kita menggunakan semua parameter optimal dari langkah sebelumnya\n",
    "from sklearn.ensemble import AdaBoostClassifier, VotingClassifier\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(dataSA[\"tweet_cleaned\"], dataSA[\"sentimen\"], \n",
    "                                                    test_size=testSize, random_state = seed)\n",
    "                                                    \n",
    "vsm = TfidfVectorizer(lowercase=True, smooth_idf= True, sublinear_tf=True, \n",
    "                                   ngram_range=(1, 1), max_df=0.95, min_df=10)\n",
    "\n",
    "x_train = vsm.fit_transform(x_train) # \"Fit_Transform\"\n",
    "x_test = vsm.transform(x_test) # Perhatikan disini hanya \"Transform\"\n",
    "\n",
    "kNN = neighbors.KNeighborsClassifier(n_neighbors=10, weights='distance')\n",
    "SVM = svm.SVC(C=0.1, gamma=1, kernel='rbf', decision_function_shape='ovr')\n",
    "ann = MLPClassifier(hidden_layer_sizes=(5, 10), learning_rate='invscaling', activation='logistic')\n",
    "\n",
    "ensemble = VotingClassifier(estimators=[('k-NN', kNN), ('SVM', SVM), ('ANN', ann)], voting='hard')\n",
    "ensemble.fit(x_train, y_train)\n",
    "y_ens = ensemble.score(x_test, y_test)\n",
    "\n",
    "print('Akurasi k-NN = ', gridCV.best_score_)\n",
    "print('Akurasi SVM = ', randsvmCV.best_score_)\n",
    "print('Akurasi ANN = ', randNnCV.best_score_)\n",
    "print('Akurasi Ensemble = ', y_ens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Akurasi k-NN =  0.4846140457982563\n",
      "Akurasi SVM =  0.5967140741811794\n",
      "Akurasi ANN =  0.5681411059700534\n",
      "Akurasi Ensemble =  0.6091581868640148\n"
     ]
    }
   ],
   "source": [
    "kNN = neighbors.KNeighborsClassifier(n_neighbors=10, weights='distance')\n",
    "SVM = svm.SVC(C=0.1, gamma=1, kernel='rbf', decision_function_shape='ovr', probability=True)\n",
    "ann = MLPClassifier(hidden_layer_sizes=(5, 10), learning_rate='invscaling', activation='logistic')\n",
    "\n",
    "ensemble = VotingClassifier(estimators=[('k-NN', kNN), ('SVM', SVM), ('ANN', ann)], voting='soft')\n",
    "ensemble.fit(x_train, y_train)\n",
    "y_ens = ensemble.score(x_test, y_test)\n",
    "\n",
    "print('Akurasi k-NN = ', gridCV.best_score_)\n",
    "print('Akurasi SVM = ', randsvmCV.best_score_)\n",
    "print('Akurasi ANN = ', randNnCV.best_score_)\n",
    "print('Akurasi Ensemble = ', y_ens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <font color=\"blue\">Contemplate</font> Keseluruhan Proses\n",
    "\n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Latihan: \n",
    "\n",
    "* Bandingkan jika kita menggunakan Tweet yang tidak di preprocessed (lagi)\n",
    "* Bandingkan jika menggunakan Stopword filtering\n",
    "* Apakah hasilnya lebih baik? Mengapa?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Masa Depan Data Mining & Software Engineering di Jaman Big Data (Science)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "# <center><font color=\"blue\">Syarat Cukup Big Data</font></center>\n",
    "\n",
    "<img alt=\"\" src=\"images/syarat_Cukup_BigData.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center><font color=\"blue\"> Perkembangan Paradigma Pemrograman\n",
    "\n",
    "<hr />\n",
    "<img alt=\"\" src=\"images/7_Functional_Prog_Meme.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Map-Reduce di Functional Programing adalah dasar pemrograman (operasi) di Big Data\n",
    "\n",
    "<img alt=\"\" src=\"images/python map-reduce.PNG\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 5, 10]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list( map(lambda x: x**2+1, [1,2,3]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[False, True, True, False, True]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list( map(lambda x: x%2!=0, [2,5,3,8,7]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5, 3, 7]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filter: bedanya jumlah elemen input (argumen) berkurang panjangnya\n",
    "\n",
    "list( filter(lambda x: x%2!=0, [2,5,3,8,7]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "data = [33, 45.9, 13, 19.2, 14.7, 32, 4, 15.1, 12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "F = lambda x: x>12 and x<20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [33, 45.9, 13, 19.2, 14.7, 32, 4, 15.1, 12]\n",
    "len( list( filter(F, data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def SUM(L):\n",
    "    s = 0\n",
    "    for e in L:\n",
    "        s = s + e\n",
    "    return s\n",
    "SUM([1,2,3,4,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reduce: F:R^M ==> R^N where  N<M\n",
    "from functools import reduce\n",
    "\n",
    "reduce(lambda a,c: a+c, [1,2,3,4,5], 0) #tak perlu list karena skalar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<p><img alt=\"\" src=\"images/7_reduce.png\" style=\"width: 556px; height: 282px;\" /></p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "120"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reduce(lambda a,c: a*c, [1,2,3,4,5], 1) # Reduce utk menghitung faktorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Map-Reduce in Fp as a basis for Big Data operations\n",
    "\n",
    "<img alt=\"\" src=\"images/mapreduce_tgrall.jpg\" style=\"width: 600px; height: 450px;\" />\n",
    "\n",
    "## Pembahasan lebih detail nantikan di https://tau-data.id/hpds/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Trend Research\n",
    "\n",
    "* Optimal Parameter on Distributed Data - Distributed Computing\n",
    "* Evaluasi Model di Big Data\n",
    "* Visualisasi di Big Data\n",
    "* Data Science/Big Data on Env-Friendly Hardware\n",
    "* Privacy Preserving DS\n",
    "* Quantum Programming (especially security/cryptography)\n",
    "\n",
    "## Selengkapnya: https://tau-data.id/dsbd-06/\n",
    "\n",
    "<img alt=\"\" src=\"images/future_tech.jpg\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center><font color=\"blue\"> End of Module\n",
    "\n",
    "<hr />\n",
    "<img alt=\"\" src=\"images/meme_4.png\" />"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
